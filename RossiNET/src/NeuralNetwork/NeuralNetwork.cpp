#include "rpch.h"
#include "NeuralNetwork.h"
#include "MathFunctions.h"

#include <ctime>

namespace NNetwork
{
	NeuralNetwork::NeuralNetwork()
	{
		// This line seeds the random number generator with the current time,
		// so that the random values generated by rand() will be different each time the program is run.
		srand(time(NULL));

		// Initialize weights and biases with random values between -1 and 1.
		weightsInputToHidden =
		{
			((double)rand() / RAND_MAX) * 2 - 1,
			((double)rand() / RAND_MAX) * 2 - 1,
			((double)rand() / RAND_MAX) * 2 - 1,
			((double)rand() / RAND_MAX) * 2 - 1
		};

		weightsHiddenToOutput =
		{
			((double)rand() / RAND_MAX) * 2 - 1,
			((double)rand() / RAND_MAX) * 2 - 1
		};

		biasHidden =
		{
			((double)rand() / RAND_MAX) * 2 - 1,
			((double)rand() / RAND_MAX) * 2 - 1
		};
		biasOutput = ((double)rand() / RAND_MAX) * 2 - 1;
	}

	NeuralNetwork::~NeuralNetwork()
	{
	}

	double NeuralNetwork::Train(const vector<vector<double>>& trainingInput, const vector<double>& targetOutput,
	                            int epochs, double learningRate)
	{
		double totalError = 0;

		// Each iteration or epoch
		for (int e = 0; e < epochs; e++)
		{
			totalError = 0;
			for (size_t i = 0; i < trainingInput.size(); i++)
			{
				//totalError = 0;
				// Calculate values for two hidden nodes in the hidden layer.
				vector<double> hiddenLayer(2);
				hiddenLayer[0] = MathFunctions::Sigmoid(
					trainingInput[i][0] * weightsInputToHidden[0] + trainingInput[i][1] * weightsInputToHidden[1] +
					biasHidden[0]);
				hiddenLayer[1] = MathFunctions::Sigmoid(
					trainingInput[i][0] * weightsInputToHidden[2] + trainingInput[i][1] * weightsInputToHidden[3] +
					biasHidden[1]);

				// Calculate output value.
				double output = MathFunctions::Sigmoid(
					hiddenLayer[0] * weightsHiddenToOutput[0] + hiddenLayer[1] * weightsHiddenToOutput[1] + biasOutput);

				// Find the error.
				double error = targetOutput[i] - output;
				//cout << "Numerical error with this input: " << error << endl;

				// Computes the delta value for the output neuron using the error and the derivative of the sigmoid() activation function.
				double deltaOutput = error * MathFunctions::SigmoidDerivative(output);
				//cout << "Delta output with this input: " << deltaOutput << endl;

				// Computes the delta values for the two neurons in the hidden layer using the delta value of the output neuron
				// and the derivatives of the sigmoid() activation function and the weights connecting the hidden layer to the output layer.
				vector<double> deltaHidden =
				{
					deltaOutput * weightsHiddenToOutput[0] * MathFunctions::SigmoidDerivative(hiddenLayer[0]),
					deltaOutput * weightsHiddenToOutput[1] * MathFunctions::SigmoidDerivative(hiddenLayer[1])
				};

				// Update the weights and biases of the neural network.
				// Uses delta values computed during backpropagation.
				for (int j = 0; j < 2; j++)
				{
					weightsHiddenToOutput[j] += learningRate * deltaOutput * hiddenLayer[j];
				}

				for (int j = 0; j < 4; j++)
				{
					int inputIndex = j % 2;
					int hiddenIndex = j / 2;
					weightsInputToHidden[j] += learningRate * deltaHidden[hiddenIndex] * trainingInput[i][inputIndex];
				}

				biasHidden[0] += learningRate * deltaHidden[0];
				biasHidden[1] += learningRate * deltaHidden[1];
				biasOutput += learningRate * deltaOutput;

				totalError += abs(error);

				// Print out the error every 1000 epochs.
				if (e % 1000 == 0)
				{
					std::cout << "Epoch " << e << ", Error: " << totalError << endl;
				}
				//cout << endl;
			}
		}
		return totalError;
	}

	double NeuralNetwork::Predict(const vector<double>& input)
	{
		vector<double> hiddenLayer(2);

		hiddenLayer[0] = MathFunctions::Sigmoid(
			input[0] * weightsInputToHidden[0] + input[1] * weightsInputToHidden[1] + biasHidden[0]);
		hiddenLayer[1] = MathFunctions::Sigmoid(
			input[0] * weightsInputToHidden[2] + input[1] * weightsInputToHidden[3] + biasHidden[1]);

		double output = MathFunctions::Sigmoid(
			hiddenLayer[0] * weightsHiddenToOutput[0] + hiddenLayer[1] * weightsHiddenToOutput[1] + biasOutput);
		return output;
	}
}




